{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0]) # ord return the unicode id of a letter.\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):      \n",
    "      batches.append(self._next_batch())    \n",
    "      \n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model.\n",
    "\n",
    "What this model does was not clear at all, so here are some temptative at understanding it.\n",
    "\n",
    "We want to create some sort of language model in which we try to predict the next character in a sentence given the previous chars of the same sentence. We try to predict one single char for the moment because that makes the vocabulary size (and the number of possible outcomes of the LSTM predictor) reasonable.\n",
    "\n",
    "The output of the character prediction model may depend on inputs that are quite distants (think about which gender to use in a sentence or whether some word should be singular or plural). This can make training quite complicated. Instead of using very long sentences as inputs, we will only consider fixed length sequences of 10 characters (parameter `num_unrollings`). This means that we assume that the next char in a sentence can't be influenced by a character that appeared more than 10 chars ago. \n",
    "\n",
    "The optimization of the parameters is made by stochastic gradient descent using batches of inputs of length 64 (i.e. each batch is made of 64 sequences of 10 characters). The loss function is the average multiclass cross-entropy between the predicted class probabilities and the true class : \n",
    "$$\n",
    "H = -\\frac{1}{N}\\sum_{i=1}^N \\sum_{j\\in \\{\\alpha\\}}y_{ij}\\log(p_{ij})\n",
    "$$\n",
    "Where $y_{ij}$ is the true label of the $i^{th}$ input, one-hot-encoded, and $p_{ij}$ is the class probability predicted for the same $i^{th}$ input.\n",
    "\n",
    "In tensorflow, class probabilities are computed using the softmax function applied to a linear classifier. However, the cross entropy is computed from the output (which are in the form of logits, i.e. varying from $-\\infty$ to $+\\infty$) in one operation. \n",
    "\n",
    "The quality of the LSTM is evaluated using a slightly different metric, perplexity, which is roughly an exponential function of the cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.295368 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.99\n",
      "================================================================================\n",
      "b ue  r h eqlr  rt aysoaonhuyqhaosnn  k  cj qv altqkowm m a cnkl lyde zspqeitjaa\n",
      "yxtijn caglllmw qjs nciqdfheoeky jmh uxt polfo   gp jkmerk v fa nd rthccye a  o \n",
      "vxjn otkux zuesbti ytao lbsmivn ieq nifp udwesinckeka  g nm dhwnckcbngpthwalgwig\n",
      "kelvspcnk qsi njszrtpamcablfnpmnmwsdnye oshbdfmlf h ee  kh  gwfcty eonenrnnlnoji\n",
      "jvsj js  hh ua e fw pwwth zzuxsedndimylwuiedl j e  dty ggoscmnseuwcp gyf knrow  \n",
      "================================================================================\n",
      "Validation set perplexity: 20.15\n",
      "Average loss at step 100: 2.596106 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.67\n",
      "Validation set perplexity: 10.03\n",
      "Average loss at step 200: 2.247379 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.47\n",
      "Validation set perplexity: 8.50\n",
      "Average loss at step 300: 2.111582 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.50\n",
      "Validation set perplexity: 8.11\n",
      "Average loss at step 400: 2.013580 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.49\n",
      "Validation set perplexity: 7.96\n",
      "Average loss at step 500: 1.945106 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.58\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 600: 1.915368 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.29\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 700: 1.867735 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Validation set perplexity: 6.55\n",
      "Average loss at step 800: 1.822774 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 6.43\n",
      "Average loss at step 900: 1.837562 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.06\n",
      "Validation set perplexity: 6.15\n",
      "Average loss at step 1000: 1.828623 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "================================================================================\n",
      "ored to the courft eight fincemed woorder pult new colvo of three unteced of cyn\n",
      "le so bere one sixn preservur ngo on is newnder edondal punted conialive the g e\n",
      "couse the yamb by winder terna lealt mealluge chact the wrint in nut of store wu\n",
      "ry of heniage lighters s ethect of being a of a wr dees of couses sicker engleli\n",
      "jeors bage rather sible hist sicue the streor med be gour an of neeper an anceam\n",
      "================================================================================\n",
      "Validation set perplexity: 6.03\n",
      "Average loss at step 1100: 1.778705 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 5.94\n",
      "Average loss at step 1200: 1.754237 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 5.76\n",
      "Average loss at step 1300: 1.735991 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 5.62\n",
      "Average loss at step 1400: 1.747046 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 1500: 1.740004 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 1600: 1.752138 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 5.54\n",
      "Average loss at step 1700: 1.714818 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 1800: 1.678611 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 1900: 1.648065 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 2000: 1.694133 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "================================================================================\n",
      "ing usion moderys the midacter special not two one nine for on the norpurish shi\n",
      "gnard sourchwin sean were sambnar peasifians perforted stied final sechinginal t\n",
      " southtual gelmably waskadochs line one nine three phy in one nine two moder in \n",
      "quediness cull and we munath on thos in two asstyning a vibli stannotony jode sc\n",
      "rankine arrotility is is schoul by monistra alghic lokngwaol alvitaction gomenca\n",
      "================================================================================\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2100: 1.686532 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 2200: 1.679922 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.46\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 2300: 1.642929 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 2400: 1.657962 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 2500: 1.679095 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 2600: 1.648238 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 2700: 1.652653 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 2800: 1.648068 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 2900: 1.648415 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3000: 1.649138 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "================================================================================\n",
      "h bativationshion long six the more wistern koun pappe remaegh fictions borks di\n",
      "hands alleficity from the dennincy pad mas logal in deforsm dury with and unshag\n",
      "pticide not booked soughtlyrar lideral indor meashifical saiventen one nine seve\n",
      "zer risch hassion decomely gritievally as in to in it add la greigh rulop intemp\n",
      "upal chalacla mary dling tounde falisaus accession popilated than life incleeny \n",
      "================================================================================\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3100: 1.626376 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3200: 1.642584 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 3300: 1.639233 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3400: 1.669407 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 3500: 1.658086 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 3600: 1.667924 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 3700: 1.644344 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 3800: 1.644913 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 3900: 1.636966 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 4000: 1.650677 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "================================================================================\n",
      "jom one nine six r the yace mol express and lamon frence servers the emprobitors\n",
      "jee ampriqued to perial wed south increted fist connan mothers alban gyozer prod\n",
      "at past remalalioad emperor wide numporms one nine one five six five fover was n\n",
      "ragy which from livils ofbitiate itsors internotyleved quartines be games as pro\n",
      "jom one the broins over crists of man remative s among interpreten possic causin\n",
      "================================================================================\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 4100: 1.633655 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 4200: 1.639255 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4300: 1.616984 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 4400: 1.606126 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 4500: 1.616344 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4600: 1.617914 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 4700: 1.623400 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 4800: 1.631695 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 4900: 1.632665 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 5000: 1.609655 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "================================================================================\n",
      "manijated sulvices on thenp than the resmuntuoly screachish nonalbed to this mod\n",
      "pext which diep nestacore even lived ittiegh c munty was al sucper une occen as \n",
      "godious be while plancing five six six costicues blacking all many system are si\n",
      "zenly that actmits compor the yausent strikd bydaws has warkity by whiles contro\n",
      "quan ha in wenter and not hannhaphy united slave protairs dugute she as a gatpe \n",
      "================================================================================\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5100: 1.609771 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5200: 1.589681 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 5300: 1.576904 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5400: 1.578408 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 5500: 1.568377 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 5600: 1.583330 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 5700: 1.573043 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 5800: 1.579310 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 5900: 1.576254 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 6000: 1.544830 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "================================================================================\n",
      "ace lifistion kage espedoss one five five three one seary lices names axeric non\n",
      "lious but the brachosoniet the nums har history will of dideignly concestrue sev\n",
      "ing a fix philas dogul was has as hightopment i is ond high hearly opyled of imp\n",
      "wedh of a adowne ironle bribity speak under that x age of the used at othero tum\n",
      "der fempt specie quactial starfility fear noted stri pono for by over earth part\n",
      "================================================================================\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 6100: 1.567594 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 6200: 1.535100 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6300: 1.545793 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 6400: 1.538368 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 6500: 1.560624 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 6600: 1.594401 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6700: 1.581256 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6800: 1.601545 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 6900: 1.582599 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 7000: 1.576156 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "================================================================================\n",
      "nisely on impossove cellion discoveral according to deturing is with had alan gn\n",
      "que the listan one nine eight zero three one nine eight six four one seven two k\n",
      "h to south offfoc hysspon five four four bra ir cultures in the price councils i\n",
      "omughwocd archessibo protting vi god the word back and offlual that skin gived i\n",
      "snachinage tion and inflations and ravied over three zero zero six aurember are \n",
      "================================================================================\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 7100: 1.572147 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 7200: 1.572007 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 7300: 1.567211 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.96\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 7400: 1.587637 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 7500: 1.590515 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 7600: 1.555233 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 7700: 1.547417 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 7800: 1.576875 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 7900: 1.585387 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.09\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 8000: 1.614628 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.31\n",
      "================================================================================\n",
      "rech temp ocesconse play bulide in one nine univer elderm of the pojer as antos \n",
      "y westrayal euriest to malomenn fint waw althrox which cocned to winter all va m\n",
      "rast the dive than noin to finage one nine five ten nution theardities of that s\n",
      "jethal every it served undosed the soon flomatity fiose such repues and caitical\n",
      "nise tegas inflasting polace conditic to person is consumptes so the d how who p\n",
      "================================================================================\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 8100: 1.591030 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 8200: 1.565700 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 8300: 1.565592 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 8400: 1.579394 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 8500: 1.576953 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 8600: 1.579810 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 8700: 1.569832 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 8800: 1.540522 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.12\n",
      "Average loss at step 8900: 1.562237 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.03\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 9000: 1.553207 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.29\n",
      "================================================================================\n",
      "jamm also zowell of batters have in decreastors of his number esporks to advance\n",
      "were organiam of second other uniders all in a perlogy people with the proviss a\n",
      "ing rungc jookriwards in clann use laffic musy nationation with sharph to the st\n",
      "py one nine i morle back stolemisu of philth good in the ribyle which awass out \n",
      "x masna semal referly national destrias closs is tradition five two whiter of th\n",
      "================================================================================\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 9100: 1.564310 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.11\n",
      "Average loss at step 9200: 1.592015 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.13\n",
      "Average loss at step 9300: 1.601626 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 9400: 1.590355 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 9500: 1.596919 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.13\n",
      "Average loss at step 9600: 1.584560 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 9700: 1.596931 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 9800: 1.596092 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 9900: 1.589534 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 10000: 1.609091 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.78\n",
      "================================================================================\n",
      "land have the litery allion use and ewndh the lates fest islandanols addly the c\n",
      "ver piees these detrigide for joke i hard and periational people artr recoidpone\n",
      "s to the million of divertions of and they govilition and last design argums mon\n",
      "dine two and number major city conscial shi drain dornal curred for planet asson\n",
      "chic nerdli true regen at monal late eluction ny sens zero three or creeted in t\n",
      "================================================================================\n",
      "Validation set perplexity: 4.15\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The LSTM behaves according to the following equations :\n",
    "$$\n",
    "\\begin{align}\n",
    "i_t&=\\sigma(W_{ix}x_t+W_{im}o_{t-1}+b_i)\\\\\n",
    "f_t&=\\sigma(W_{fx}x_t+W_{fm}o_{t-1}+b_f)\\\\\n",
    "c_t&=f_t\\odot c_{t-1}+i_t\\odot\\tanh(W_{cx}x_t+W_{cm}o_{t-1}+b_c)\\\\\n",
    "o_t&= \\sigma(W_{ox}x_t+W_{om}o_{t-1}+b_o)\\\\\n",
    "y_t&=w(o_t\\odot\\tanh(c_t))+b\n",
    "\\end{align}\\\\\n",
    "$$\n",
    "\n",
    "where :\n",
    "- $x_t$ is the input to the LSTM cell.\n",
    "- $o_t$ is the output to the LSTM cell.\n",
    "- $f_t$ is the output of the forget gate.\n",
    "- $c_t$ is the state of the cell.\n",
    "- $o_t$ is the output of the output gate.\n",
    "- $y_t$ is the output of the classifier.\n",
    "- $\\odot$ denotes the Hadamard (element-wise) matrix product.\n",
    "\n",
    "The first four equations contains the same matrix products :\n",
    "$$\n",
    "a_t \\sim W_{ax}x_t+W_{am}o_{t-1}+b_a.\n",
    "$$\n",
    "where the matrices have the following dimensions :\n",
    "- $W_{ax}$ : vocabulary_size x num_nodes\n",
    "- $x_{t}$ : vocabulary_size x 1\n",
    "- $W_{am}$ : num_nodes x num_nodes\n",
    "- $o_{t-1}$ : num_nodes x 1\n",
    "We could compute the matrix product first in one matrix multiplication, and then extract some subcomponents of the matrix to apply the appropriate activation function :\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\tilde{i}_t\\\\\n",
    "\\tilde{f}_t\\\\\n",
    "\\tilde{c}_t\\\\\n",
    "\\tilde{o}_t\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "W_{ix}& W_{im}\\\\\n",
    "W_{fx}& W_{fm}\\\\\n",
    "W_{cx}& W_{cm}\\\\\n",
    "W_{ox}& W_{om}\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "x_t\\\\\n",
    "o_{t-1}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "The reason to perform one matrix multiplication rather than four is not clear to me, but I assume it has to do with tensorflow being faster at one big matrix multiplication rather than four small ones... I could check it out in a benchmark.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  wx = tf.Variable(tf.truncated_normal([vocabulary_size, 4*num_nodes], -0.1, 0.1))\n",
    "  wm = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "  wb = tf.Variable(tf.zeros([1, 4*num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    matprod = tf.matmul(i, wx) + tf.matmul(o, wm) + wb\n",
    "    matprod_input, matprod_forget, update, matprod_output = tf.split(matprod, 4, 1)\n",
    "    input_gate = tf.sigmoid(matprod_input)\n",
    "    forget_gate = tf.sigmoid(matprod_forget)    \n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(matprod_output)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "                                saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.297404 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.04\n",
      "================================================================================\n",
      "jiftqtdtjbhitfftzlqf dfz r   ujyh nmnditjmcapxkovvvrap vbow  wtrj tetfenxkmyio o\n",
      "b d ekyjtl fifzw sscuageiedswfxidqoapgiq nz tyd rmnoz ncgdtmxvnviudgz dmc   rytd\n",
      "izmde tbuemw vwn dhoeb   mafaunu ae uw  trthsw fx yr yvtiiebtnnytbiliegddqs zge \n",
      "whedeecte cnoq gwdmtesulqtssd sa    ojaxrflsq brhguovolpxo l c ztsh zrseaokfefes\n",
      "t oxteoc cbotpppxqvy kv p tnqmisst jep j zwferohhnex iifi rda glbyzsjg su bj rxq\n",
      "================================================================================\n",
      "Validation set perplexity: 20.34\n",
      "Average loss at step 100: 2.594014 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.03\n",
      "Validation set perplexity: 10.32\n",
      "Average loss at step 200: 2.250141 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.58\n",
      "Validation set perplexity: 8.70\n",
      "Average loss at step 300: 2.105247 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.60\n",
      "Validation set perplexity: 8.16\n",
      "Average loss at step 400: 2.005316 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.58\n",
      "Validation set perplexity: 7.79\n",
      "Average loss at step 500: 1.943190 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.63\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 600: 1.912089 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 700: 1.864191 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 6.47\n",
      "Average loss at step 800: 1.824151 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 900: 1.838266 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.06\n",
      "Validation set perplexity: 6.17\n",
      "Average loss at step 1000: 1.833582 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "================================================================================\n",
      "ully defiltions in one nine revabl in one six exteing crovelto lect in one nine \n",
      "qu elast marical palle of the hecenver the sikn state acourly staquus president \n",
      "h seuring to ownlions rovictems to whelm beliz oppented d eadenic daynied  mmide\n",
      "e wikl of a pridervy the pupulipon neail of the conselog s fumseis stowel his re\n",
      "ze to n the one nine nine five zero three of alex fromotion in ofjent wohs nine \n",
      "================================================================================\n",
      "Validation set perplexity: 5.99\n",
      "Average loss at step 1100: 1.780649 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.77\n",
      "Average loss at step 1200: 1.757626 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 5.58\n",
      "Average loss at step 1300: 1.736425 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 1400: 1.747761 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 1500: 1.741756 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 1600: 1.752113 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 1700: 1.714389 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 1800: 1.680512 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 1900: 1.649239 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 2000: 1.699485 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "================================================================================\n",
      "s and hopb was fave tabedborn dubilian so is deriving abander by tlading is amer\n",
      "cut the refunistion with the expernos relegnent as guguap dismass lake ensimpor \n",
      "xuatius bat to the land the mades possidring first curtablisabas instutstay of t\n",
      "y p bown characen the endibyen fron extactives campless the norcing bety copn du\n",
      "might to pespictisa sheeding expere swo v revisting legary excluction singwronob\n",
      "================================================================================\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 2100: 1.684537 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 2200: 1.683528 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.29\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 2300: 1.642261 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 2400: 1.661969 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 2500: 1.680664 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 2600: 1.654312 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 2700: 1.658383 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 2800: 1.654519 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 2900: 1.653091 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 3000: 1.652197 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "================================================================================\n",
      "ban transs quet of two one scanslation involit fordegation of the gave indeg led\n",
      "per saif from victorn mata the has couther offless as cansasmerly which enrect t\n",
      "le an seabarge one five now hesticully ancouldis stand ablicked bouts is court v\n",
      "scalas pays of others as emperor sevenden over the unlay transterwalded include \n",
      "nend of bre che toke agonnly with as their coarkan as the calaman netwoll othed \n",
      "================================================================================\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 3100: 1.631592 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 3200: 1.645830 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3300: 1.639270 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 3400: 1.669732 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3500: 1.657308 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3600: 1.668556 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 3700: 1.649537 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3800: 1.651228 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3900: 1.637344 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 4000: 1.654094 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "================================================================================\n",
      "usuaut mahsion of the theo light by d four the mectine withing his refue of arnt\n",
      "phecity beforus are juiges the neading gabless develtata of symetician wnoh the \n",
      "bulk crankale arel maroas maitided express of the bevide ob the orperiations as \n",
      "z goosgally boo the pact was for a smy itaze franking comps the flugz two nince \n",
      "z and the biffor bass his seek first and pergoroginent buts rinizahin locoda use\n",
      "================================================================================\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4100: 1.631779 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4200: 1.637116 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 4300: 1.617789 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 4400: 1.607986 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 4500: 1.615920 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4600: 1.615980 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 4700: 1.630691 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 4800: 1.630285 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4900: 1.632766 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5000: 1.609792 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.44\n",
      "================================================================================\n",
      "ness noty is one nine three two eight five nine five zero e other which noon and\n",
      "rigin some tybae its notion with kashing war with sop musiclatixs eight unmany u\n",
      "form orchest for juak the fleakex of the using one nine  kspe one junity the sev\n",
      "ht impergaisly levaries the maras most the oppatter a netip overemide bulldorine\n",
      "arch for second traditionalism instalm senft eight six pulishes lite of phanting\n",
      "================================================================================\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5100: 1.607896 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 5200: 1.595291 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5300: 1.582167 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5400: 1.585850 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5500: 1.571300 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 5600: 1.583074 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5700: 1.568359 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5800: 1.584471 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5900: 1.578552 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6000: 1.551271 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "================================================================================\n",
      "gane world to a publishooblais there affus zay anart formed it define the soit h\n",
      "for canack the is two zero mirlons hillland changraph world cabites die hearly t\n",
      " called the backing of the arches a also world wrokent verson astrowed a two hed\n",
      "unity freed for the orchyinal sustheps one one one one one seven two two and cla\n",
      "hark intivenlin serval birlines frimm englv and age had booght child emist store\n",
      "================================================================================\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6100: 1.565861 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6200: 1.535204 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6300: 1.545061 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6400: 1.537544 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6500: 1.555574 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6600: 1.597215 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6700: 1.580359 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6800: 1.605071 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6900: 1.581740 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 7000: 1.577881 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "================================================================================\n",
      "s gre noviatments it we refore as a ust site taylew x most s novealistry similar\n",
      "s scone two in when their thure and presideng high fuenway encich dimar the rest\n",
      "emplain not indackon is good hypolors of generation corn distordes were loup chi\n",
      "xumull erend in the shookeened be man have network than a wases takes can foqu y\n",
      "but froms leaveds tranee year acruled mestrute slatinald society kuster of the s\n",
      "================================================================================\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 7100: 1.574769 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 7200: 1.571915 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 7300: 1.571527 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.92\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 7400: 1.586452 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 7500: 1.592196 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 7600: 1.556353 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 7700: 1.554344 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 7800: 1.579859 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 7900: 1.583223 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.01\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 8000: 1.621199 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.36\n",
      "================================================================================\n",
      "menicce to the mince harger one sigies of this and irely charchails a rikary spa\n",
      " ballabidebalso in synid to conflarary p peavers and distants yeabor ma lembical\n",
      "s junietined a legandsless aprocomes niger howinzer with through s phorines the \n",
      "man seven dimendy iss scient conture systain with edia detected there of controp\n",
      " abour sussentsed to grestbeg yanduamand musiantelle a marked and of misic to by\n",
      "================================================================================\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 8100: 1.593873 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 8200: 1.569366 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 8300: 1.567239 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 8400: 1.583042 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 8500: 1.579914 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 8600: 1.576956 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 8700: 1.570618 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 8800: 1.543362 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 8900: 1.561630 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.04\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 9000: 1.555418 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "================================================================================\n",
      "ents of historical presaning of controlley showly portedions an of one eight etr\n",
      "on die recome cogrepoly layer of argilung supperse the drectory on assoprace a i\n",
      "liea in a had love througutarest pagn code the labaling u f g entire mee private\n",
      "d annet lif two power debsivet an ising of kinkemime house pacraries magnia mema\n",
      "kotathes with those that his word caves to s his dome that abserves in south of \n",
      "================================================================================\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 9100: 1.564455 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 9200: 1.594532 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 9300: 1.604825 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 9400: 1.590923 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 9500: 1.598075 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 9600: 1.577887 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 9700: 1.599028 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 9800: 1.600120 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 9900: 1.593847 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 10000: 1.611631 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.73\n",
      "================================================================================\n",
      "on the from the wased that lozems of frequence veri three nine eight norwartra a\n",
      "vilved and is s the partine the gerombine les policiu pier to substreas into thi\n",
      "no ruth donswate setwah four one six to a contripurtions serviasine one zero zir\n",
      "h perptor degranted to hoesondebreginations and but sife madinetry companer perf\n",
      "nt now series durns achimor roman mingrootap six mariesuast raring leorlrin back\n",
      "================================================================================\n",
      "Validation set perplexity: 4.23\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Note that perplexity is slightly worse (4.23 w.r.t 4.15 before)... this is strange, because it is strictly equivalent mathematically speaking. I have no clue whether this is a meaningful difference, it might be a statistical variation..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "a. Before rushing to the bigram model, we will first modify the LSTM model to introduce an embedding layer. Usually, the embedding layer should be of lower dimension compared to the input, so we will use an embedding of size 20, which is only slightly smaller than the original vocabulary size (26). Embedding is performed in tensorflow by means of the function `tf.nn.embedding_lookup`. (see https://github.com/rndbrtrnd/udacity-deep-learning/blob/master/6_lstm.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 9\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  wx = tf.Variable(tf.truncated_normal([embedding_size, 4*num_nodes], -0.1, 0.1))\n",
    "  wm = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "  wb = tf.Variable(tf.zeros([1, 4*num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    matrix_product = tf.matmul(i, wx) + tf.matmul(o, wm) + wb\n",
    "    input_gate = tf.sigmoid(matrix_product[:,:num_nodes])\n",
    "    forget_gate = tf.sigmoid(matrix_product[:,num_nodes:2*num_nodes])\n",
    "    update = matrix_product[:,2*num_nodes:3*num_nodes]\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(matrix_product[:,3*num_nodes:])\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    input_embed = tf.nn.embedding_lookup(embeddings, tf.argmax(i, dimension=1))\n",
    "    output, state = lstm_cell(input_embed, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  sample_input_embedding = tf.nn.embedding_lookup(embeddings, tf.argmax(sample_input, dimension=1))\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.304521 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.24\n",
      "================================================================================\n",
      "z fye xueesnqqara nofg eyheuspj hwvml   mmfsimq sywvpy weowl gsrmo hmvvglnvqw lw\n",
      " teecgwhozwxnpox stp poafwc dic qbttnvsjakoo ciclhzuw  nwvi m eoton pt oly  vwjy\n",
      "kzwkc t tmxcks pa ejj ay btwoy htoulent  n e ttafqegtkp rocvjzo e o lxsl hmio  t\n",
      "jtyo n x   ea hrzre  a  hla ekrepya ob uzhebhhdff  ao amt e h utnef okanhrjly u \n",
      "wgi awit peeesiotgnsnsaoyvfiaog   oi wxsaejjd  onsh qd htxd  p e mwsa gzep voq i\n",
      "================================================================================\n",
      "Validation set perplexity: 20.03\n",
      "Average loss at step 100: 2.450120 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.78\n",
      "Validation set perplexity: 9.98\n",
      "Average loss at step 200: 2.121403 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.93\n",
      "Validation set perplexity: 8.56\n",
      "Average loss at step 300: 2.012642 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.39\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 400: 1.935842 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.66\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 500: 1.931592 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.55\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 600: 1.872374 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Validation set perplexity: 6.34\n",
      "Average loss at step 700: 1.854569 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.44\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 800: 1.833192 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.83\n",
      "Validation set perplexity: 6.16\n",
      "Average loss at step 900: 1.809436 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 1000: 1.796284 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "================================================================================\n",
      "py desintune bud fabnorm forcation of the infres lamona direcatients of greated \n",
      "ers itdal three whine some commenting produte hums the first gagamrants comperte\n",
      "ole of experation invertan from gulian his scnament pasted to utraiss the excomp\n",
      "hims inb and bbobutian name fvoro in are duth latfaftory proporer and scwensy sc\n",
      "e guen the hassion kade one firg with the sturthuils caply awnastle the leptem t\n",
      "================================================================================\n",
      "Validation set perplexity: 5.86\n",
      "Average loss at step 1100: 1.757228 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 5.60\n",
      "Average loss at step 1200: 1.737411 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 1300: 1.744324 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.29\n",
      "Validation set perplexity: 5.64\n",
      "Average loss at step 1400: 1.725182 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.08\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 1500: 1.752588 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 1600: 1.736986 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 1700: 1.725827 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 5.60\n",
      "Average loss at step 1800: 1.713294 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 1900: 1.718906 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 2000: 1.710750 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "================================================================================\n",
      "fitiors de of this usipists b spees alocpean are instance h musice pression ophi\n",
      "al deal statity includeation of compuc exrested his in itlologo rr driginan cons\n",
      "logary two zero group sever zero one nine three mistate a spearing two zero teis\n",
      "ced the nationoly some toturity i numzation chill itss from praxs beate cult tha\n",
      "uring such which the incertain in suphar playic of isternalt orly to even for pl\n",
      "================================================================================\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 2100: 1.696992 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 2200: 1.696286 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 2300: 1.700861 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 2400: 1.696251 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2500: 1.694282 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 2600: 1.663518 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 2700: 1.672135 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 2800: 1.690011 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 2900: 1.671112 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 3000: 1.672432 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "================================================================================\n",
      "athialite gampation in one five by hambuug thirs most of three externgrebuch is \n",
      "warmants is the fations by anysery its rathinary as ef guithtrones ofberaton ter\n",
      "zorned joto the three bimon s its expative boarchet oftenationale bili clocratio\n",
      "siden overmas purtil warhles proimed materes icasing aca to officy uses increame\n",
      "ns withers throwbloship the beass chain stosed the sti territies netwer it of st\n",
      "================================================================================\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 3100: 1.665109 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 3200: 1.676919 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 3300: 1.670092 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 3400: 1.655876 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 3500: 1.637865 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 3600: 1.681013 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 3700: 1.652891 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 3800: 1.655939 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 3900: 1.659485 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 4000: 1.679317 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "================================================================================\n",
      "wer the genency and joss conventing point mycomes time comp ucensi spiel have ge\n",
      "lowing two five one six hany ample philab of these by zeromenhent voltion the re\n",
      "epes as time in mix linkster feulitial southerly slelohyn britine terhain in bel\n",
      "nete and from evaplee and coin tenum leogin projulturednt courercen howeved the \n",
      "jemed informed ven unibstem univitions of the aefter four walie wik the has as b\n",
      "================================================================================\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 4100: 1.672598 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 4200: 1.650172 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 4300: 1.631249 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 4400: 1.635684 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 4500: 1.676311 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 4600: 1.649213 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 4700: 1.652385 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 4800: 1.664229 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 4900: 1.686370 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 5000: 1.613825 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "================================================================================\n",
      "bing jasically remanoar a set of all the desert coaning the cardislisty sempise \n",
      "k and septect of ecpout of the the chara one one angewend propth relation pose s\n",
      "y loter cr develessed for u s side lat casered o there communionwenks of pevarie\n",
      "k impuals rathers clot in the first on the monotry the ulandame symaliry mart su\n",
      "quibilister by back were films weigh in the when the seems furs elect fore of ti\n",
      "================================================================================\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 5100: 1.576054 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 5200: 1.606598 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 5300: 1.589404 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.05\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 5400: 1.582630 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 5500: 1.574548 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 5600: 1.587681 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 5700: 1.613348 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 5800: 1.583013 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 5900: 1.600324 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 6000: 1.609222 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.61\n",
      "================================================================================\n",
      "ythraph exegi add is have edularl included with unitere forces companting compan\n",
      "us estitional of the evebish is a tourrum a new six six concerto of roc many gro\n",
      "ria e cologing the the will welbersing world mans rown jide years and operations\n",
      "visus brew sersion that of the noth large as also gwongins programs ire chumber \n",
      "bos with simpl attenners of the name bride but of cnersinis of chmsted to rosel \n",
      "================================================================================\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 6100: 1.605667 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 6200: 1.577197 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 6300: 1.596069 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 6400: 1.593916 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 6500: 1.627037 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 6600: 1.605231 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 6700: 1.615375 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 6800: 1.583457 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 6900: 1.591698 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 7000: 1.595306 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "================================================================================\n",
      "hamad to the recordition diversing of the good there surre andume to the redieit\n",
      "quired in marks was six two  three seven zero d ttatence they lackerry varrable \n",
      "k recondert jatone the wrenstwas claimed supqubell recessor crobinal these skys \n",
      "nance are genettinc of carsolage of into most dicericture have two twer of gover\n",
      "k of the rate westers a rulguie that the resoller their publicle is pref a subur\n",
      "================================================================================\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 7100: 1.592988 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 7200: 1.586073 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 7300: 1.573382 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 7400: 1.573526 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 7500: 1.573504 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 7600: 1.575544 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 7700: 1.583571 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 7800: 1.571876 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 7900: 1.569587 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 8000: 1.581988 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "================================================================================\n",
      "ge the otting on three zero nine nine zero three zero emby idestmed com politica\n",
      "von with a mills whater i dutc to to and not of the irish amoconyds harps all as\n",
      "y one nine three two eight ed facted of cersements vecent to not pawend outsiono\n",
      "ts in fribably of northewn rung to tama statues engin one five six zero vockropi\n",
      "fise of five one nine whires arto towns not was stroned c day s kingsumple ameri\n",
      "================================================================================\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 8100: 1.576560 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 8200: 1.566839 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 8300: 1.556092 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 8400: 1.581949 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 8500: 1.593631 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 8600: 1.592198 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 8700: 1.566299 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 8800: 1.553905 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.93\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 8900: 1.568636 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 9000: 1.589864 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "================================================================================\n",
      " there againsting in all howebil excoderio hoyer made irish unfords dedoo prothi\n",
      "raw view engly min hapbot team clisfems his the minge selan dyfelled word mich h\n",
      "ws orucan mondse and visi mabaning of see this of zed to the music sochity inter\n",
      "pys who chamt extennth are qy or propals people woure haxten and agres alchadexb\n",
      "k bands an in bload other clussed four nuthaminals discow starts the mined consi\n",
      "================================================================================\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 9100: 1.623534 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 9200: 1.619491 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 9300: 1.580921 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 9400: 1.578717 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 9500: 1.568438 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 9600: 1.599090 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 9700: 1.558457 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 9800: 1.554342 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.13\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 9900: 1.576160 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 10000: 1.544500 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.32\n",
      "================================================================================\n",
      "ity in  caurie of the sonation in lifom two also intred rays which film sea the \n",
      "bor disk by five eight three three three saykst mblut used of lance fra ve brita\n",
      "nized to most thirdination a bornly sacies are is night end write of more partur\n",
      "an recort united and thebor seals or reacture gamlions there considered to zicgo\n",
      "tics of order the locut fiohecter clusing decrpc of its churnest first pring bec\n",
      "================================================================================\n",
      "Validation set perplexity: 4.58\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "summary_frequency = 100\n",
    "\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Note that validation perplexity is a bit worse, which was kind of expecteed this time...\n",
    "\n",
    "b. Now we can move forward to bigrams models...\n",
    "The idea here is to predict one character given the two previous ones. To achieve this, we need to twist the tensorflow graph so that the inputs are two characters, and the labels are the input, shifted by two chars (instead of one). We will again modify the above code.\n",
    "\n",
    "Note that in this case, we will use a bigger embedding size, for instance 64.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 128\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # MODIF 1 : the input size is now vocabulary_size^2.  \n",
    "  embeddings = tf.Variable(tf.random_uniform([vocabulary_size*vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  wx = tf.Variable(tf.truncated_normal([embedding_size, 4*num_nodes], -0.1, 0.1))\n",
    "  wm = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "  wb = tf.Variable(tf.zeros([1, 4*num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    matrix_product = tf.matmul(i, wx) + tf.matmul(o, wm) + wb\n",
    "    input_gate = tf.sigmoid(matrix_product[:,:num_nodes])\n",
    "    forget_gate = tf.sigmoid(matrix_product[:,num_nodes:2*num_nodes])\n",
    "    update = matrix_product[:,2*num_nodes:3*num_nodes]\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(matrix_product[:,3*num_nodes:])\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  # MODIF 2 : training data are now tuples of chars, instead of single char. We will convert the tuples in a unique id below. \n",
    "  train_chars = train_data[:num_unrollings]\n",
    "  train_inputs = zip(train_chars[:-1], train_chars[1:])\n",
    "  # MODIF 3 : labels are inputs shifted by two time-steps.\n",
    "  train_labels = train_data[2:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    # MODIF 4 : we compute a unique id for each bigram. bigram_id varies from 0 to 675    \n",
    "    bigram_id = tf.argmax(i[0], dimension=1) + vocabulary_size * tf.argmax(i[1], dimension=1)    \n",
    "    input_embed = tf.nn.embedding_lookup(embeddings, bigram_id)\n",
    "    output, state = lstm_cell(input_embed, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "\n",
    "  # MODIF 5 : We also need to adapt the input of the validation set. Inputs are now bigrams instead of single chars. Note, that \n",
    "  # currently, the batch generator generates batches of two chars. For the bigrams model, we will need to modify it to generate\n",
    "  # three chars (2 for inputs, 1 for output)\n",
    "  sample_input = list()\n",
    "  for _ in range(2):\n",
    "    sample_input.append(tf.placeholder(tf.float32, shape=[1, vocabulary_size]))\n",
    "  samp_in_index = tf.argmax(sample_input[0], dimension=1) + vocabulary_size * tf.argmax(sample_input[1], dimension=1)\n",
    "  sample_input_embedding = tf.nn.embedding_lookup(embeddings, samp_in_index)\n",
    "\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.306387 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.29\n",
      "================================================================================\n",
      "iat qbtifk oentrl ftduhdsyoqaxiewxqs grjthkgk qbdcyaqiegqcdpclrtbvidrslzaeaznitrr\n",
      "tzarwiphoehl stlllezoj ecpd ska  yv  so akcaa bazkwmbpnxcidwxsigmv nnvxioqksogitq\n",
      "dxnfrhvd gsiuqswcnursrhidlo dadv a pevaerr o  r evakltka  cs rvm xbaietzkpctxoweu\n",
      "pqpspto yeoxs jz mmnipevscnhhpgyulz eimplesnljnvmscongjfhgmkh millniraagsb  oaeav\n",
      "dmminnqdste acwwaa aftaaxkzy tu np eyhxacubnb sfaksb bref yapsrtfoavnqupzfh  aenu\n",
      "================================================================================\n",
      "Validation set perplexity: 19.60\n",
      "Average loss at step 100: 2.287629 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.14\n",
      "Validation set perplexity: 9.22\n",
      "Average loss at step 200: 1.944707 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 8.30\n",
      "Average loss at step 300: 1.877038 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 7.88\n",
      "Average loss at step 400: 1.806087 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 7.59\n",
      "Average loss at step 500: 1.820531 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 7.49\n",
      "Average loss at step 600: 1.774520 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 700: 1.755121 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 800: 1.747503 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 7.42\n",
      "Average loss at step 900: 1.737200 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 7.40\n",
      "Average loss at step 1000: 1.712015 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "================================================================================\n",
      "rfapinal religized whe userstroundergty peterform that ofmno music scial and cano\n",
      "iep of pol  in an and restic and mainey armed unliev usy the residenfino z the fr\n",
      "krated nomisodgelend the country submlation of do thei nominish orgian compted m \n",
      "pjill pastrave that the isplical vanetwer milithe one nine seven zero genecogthou\n",
      "buch and bning and and prossed north famisree conflicauregionslow mesigrativet sq\n",
      "================================================================================\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 1100: 1.687004 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 1200: 1.677328 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 1300: 1.677406 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 1400: 1.675795 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 1500: 1.687114 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 1600: 1.659001 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 1700: 1.667839 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 1800: 1.655227 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 1900: 1.660456 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 6.74\n",
      "Average loss at step 2000: 1.654749 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.30\n",
      "================================================================================\n",
      "jr a blastes to baring form service rengday automb hund and ething the one gatal \n",
      "daitation the family there vacire words reign georgia dical person galogography o\n",
      "ding lone sp in lme to the stated would due fireibilln afforts a trubs been herom\n",
      "ogia nortrucle chence pecinevanch was borch resentill on the figenry goesvtmolate\n",
      "xxon the episeasition fnincy know wholattems properational and one nine mr it i t\n",
      "================================================================================\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 2100: 1.648626 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 2200: 1.645292 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 2300: 1.646211 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 2400: 1.638214 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 2500: 1.640749 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 2600: 1.612541 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 7.43\n",
      "Average loss at step 2700: 1.620544 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 2800: 1.646593 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 2900: 1.619487 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 3000: 1.626900 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "================================================================================\n",
      "xly on the eventylres such describle of subsion the baosi volti many liday of one\n",
      "uq evansulatable city ascened looatre latic in ghang compted the spanishiroppinse\n",
      "fmonarfs the lands duris and back the musiiin cordiyel prograd for run one can oo\n",
      "jject right it clistical have parties a tanes ranger symbeat as the archit and th\n",
      "ew eliji strainess motion of then the normaabidaanc d bart minectance anothered b\n",
      "================================================================================\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 3100: 1.614877 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 3200: 1.621238 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 3300: 1.619988 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 3400: 1.605815 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 3500: 1.586197 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 3600: 1.630152 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 3700: 1.607402 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 3800: 1.620513 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 3900: 1.596085 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 4000: 1.623553 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "================================================================================\n",
      "a king publine eight zero one and one to nine three the vergillas franchard enera\n",
      "ving the guerries through area in coosper cheserder and the luds simplexess never\n",
      "fd healled commuturt in pwlcis on the are the lortuant the developedia wily to ei\n",
      "qted similtable derchieff also passe state abvs group memovre stalies on the foun\n",
      "il becausing sent chard sries compley ferelia for one five one ay b the some inci\n",
      "================================================================================\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 4100: 1.630994 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 4200: 1.582147 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 4300: 1.575451 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 7.45\n",
      "Average loss at step 4400: 1.587314 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 7.52\n",
      "Average loss at step 4500: 1.621514 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 4600: 1.595706 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 4700: 1.601012 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 4800: 1.627023 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 4900: 1.621486 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 7.43\n",
      "Average loss at step 5000: 1.557646 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "================================================================================\n",
      "xce litz laboat theosness used and these some british in rank of it in sabour sta\n",
      "out which derior also design near and co maker new ward long raviding while poisa\n",
      "ahilb the indits film inflar are acvxnam the mystables berna anothers of plot epo\n",
      "ds julitude working of the war ross scevers act or orninool geneth respon in the \n",
      "apped set ensations with preb are plant it week one eight data milleter status ni\n",
      "================================================================================\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 5100: 1.561263 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 5200: 1.584743 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 5300: 1.557528 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 5400: 1.559328 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 5500: 1.557086 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 5600: 1.569385 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 5700: 1.598199 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 5800: 1.558292 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 5900: 1.575848 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 6000: 1.596281 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "================================================================================\n",
      "synted not giventional can topter fille ambrose formal but s but for public of be\n",
      "ide symbocioles that the in the the humany cyany no this converter this to the hi\n",
      "iya two zero zero five s ork fors common was cakes one five seven two four zero z\n",
      "jyoner wii in infgmens and the simplies the brag ewith know with addition in repr\n",
      "umbig providist of the used three ingenized carries the batto physical visite oth\n",
      "================================================================================\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 6100: 1.574604 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 6200: 1.557901 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 6300: 1.563852 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 6400: 1.585631 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 6500: 1.601438 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 6600: 1.576146 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 6700: 1.591746 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 6800: 1.556987 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 6900: 1.556749 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 7000: 1.578222 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.32\n",
      "================================================================================\n",
      "ection it  upon in one nine eight is naturals as centribil one seven zero distrom\n",
      "nmental states as functioned ther banking instation tleign nuclauda warmody two h\n",
      "dp selebelly actor and so rober to the take was andsobablys it rogegalitism in th\n",
      "qps and one nine zero fligwoes replace in and such its movin ket has special head\n",
      "gcg mcnapipe well delecting four seven one seven mgre clam sinated to not lyright\n",
      "================================================================================\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 7100: 1.564332 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.30\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 7200: 1.568944 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 7300: 1.553059 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 7400: 1.550938 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 6.56\n",
      "Average loss at step 7500: 1.551004 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 7600: 1.559834 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 7700: 1.550545 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.31\n",
      "Validation set perplexity: 6.57\n",
      "Average loss at step 7800: 1.545382 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 7900: 1.543743 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 8000: 1.559039 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "================================================================================\n",
      "gv its italier wrot select in researchesions but not d temptic cronir factorsy or\n",
      "sxly busined sports in choory batche subsequent itselfather followed science so g\n",
      "lqcs software in the toadical talast often capipact of spaces for these who nine \n",
      "tpace the county sceptuty who jenge discorport in at knover or r urder animed tha\n",
      "jc irmy top willed to from that the some belord  usually the indic industry they \n",
      "================================================================================\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 8100: 1.548139 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 8200: 1.535543 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 8300: 1.528184 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.19\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 8400: 1.576655 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 8500: 1.565430 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 8600: 1.563476 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 8700: 1.535877 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 8800: 1.527705 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 8900: 1.548442 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 9000: 1.576527 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "================================================================================\n",
      "eps as the came american chemisning preceficion position fertical not sweget hous\n",
      "impers increasonal scand a recognal was is fungen simple most with a mail house r\n",
      "the wild by strument framents may dc gozamed in refinity gusk of cubach volcmt ef\n",
      "gket think to are of names itself its united for opera as in one nine two zero ch\n",
      "ce a pressive is use these in phily in bell an film him becotie politicy differic\n",
      "================================================================================\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 9100: 1.577332 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 9200: 1.586576 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 9300: 1.565574 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 9400: 1.549955 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.14\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 9500: 1.542937 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 9600: 1.566984 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.26\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 9700: 1.531412 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 9800: 1.531599 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.00\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 9900: 1.551083 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 10000: 1.534630 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.48\n",
      "================================================================================\n",
      "xp imprize deciliam mystive berthman law people that his times of the robertia th\n",
      "poly and successfully that cont the litputes with neore indus explorad the numbel\n",
      "ic by is jop hells get dras troached i leviegaty official back ammon the pather t\n",
      "wqas soldies up rabout zero zero bands cive amem the localinder rake deet years h\n",
      "xplain has in the chriruject clos a toll develobal gram unsisitions if d an in re\n",
      "================================================================================\n",
      "Validation set perplexity: 6.90\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "summary_frequency = 100\n",
    "# MODIF 6 : Here we change the Batchgenerator to have three chars instead of 2. \n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      # MODIF 6 : labels are inputs shifted by two chars, not one\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      # MODIF : the text generator takes one letter at random and generate a sentence. We need it to take on bigram at random. \n",
    "      # I will do this later.   \n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "      # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          #MODIF : Sample two random chars\n",
    "          sentence=str()\n",
    "          input =[]\n",
    "          for _ in range(2):\n",
    "            feed = sample(random_distribution())\n",
    "            input.append(feed)\n",
    "            sentence+=characters(feed)[0]\n",
    "             \n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input[0]: input[0],\n",
    "                                                 sample_input[1]: input[1]})\n",
    "            feed = sample(prediction)\n",
    "            input[0]=input[1]\n",
    "            input[1]=feed\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({\n",
    "                    sample_input[0]: b[0],\n",
    "                    sample_input[1]: b[1]\n",
    "            })\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
